# -*- coding: utf-8 -*-
"""FindingRice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XtN_CC6ik-cle6QFLEnZdWko1hbu05bk
"""

# ==============================================================================
# üöÄ Module 1: Unified YOLOv8 Training System (Economy Rice Smart Checkout)
# ==============================================================================

# 1. Environment Setup
print("üöÄ Installing YOLOv8 environment...")
!pip install ultralytics -q  # Uncomment if running in a fresh Colab environment

import os
import shutil
import yaml
import glob
import zipfile
from ultralytics import YOLO

# ---------------------------------------------------------
# ‚öôÔ∏è Configuration Area
# ---------------------------------------------------------
# Name of your zip file (without the .zip extension)
DATASET_NAME = 'food_dataset'

# ‚ö†Ô∏è CRITICAL: The order of these classes MUST match the class IDs in your Roboflow export!
# Updated to include the 'plate' class based on your latest dataset requirements.
# Assuming YOLO uses 0-indexing (0=meat, 1=rice, 2=vege, 3=plate)
CLASSES = ['meat', 'rice', 'vege', 'plate']

# ---------------------------------------------------------

ZIP_PATH = f'/content/{DATASET_NAME}.zip'
DATASET_DIR = f'/content/{DATASET_NAME}'

print(f"\n\n{'='*50}")
print(f"üî• Initializing Multi-Class Unified Training Pipeline")
print(f"{'='*50}")

# Clean up old data to prevent conflicts
if os.path.exists(DATASET_DIR):
    shutil.rmtree(DATASET_DIR)

# Safe Extraction
if not os.path.exists(ZIP_PATH):
    print(f"‚ùå Error: {ZIP_PATH} not found! Please check if it is uploaded in the left panel.")
else:
    print(f"üì¶ Safely extracting {DATASET_NAME}.zip ...")
    # Replaced OS command with native Python zipfile for better stability
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(DATASET_DIR)

    # Smartly locate the images directory
    train_dir = None
    val_dir = None

    for root, dirs, _ in os.walk(DATASET_DIR):
        if 'images' in dirs:
            full_path = os.path.join(root, 'images')
            if 'train' in root:
                train_dir = full_path
            elif 'valid' in root or 'val' in root:
                val_dir = full_path

    # Fallback search strategy
    if not train_dir:
        potentials = glob.glob(f"{DATASET_DIR}/**/train/images", recursive=True)
        if potentials: train_dir = potentials[0]

    if not val_dir and train_dir:
        print(f"‚ö†Ô∏è Warning: Validation set (valid/val) not found. Using training set as fallback.")
        val_dir = train_dir

    if not train_dir:
        print(f"‚ùå Fatal Error: Could not find 'train/images' folder in the zip file! Please check dataset structure.")
    else:
        # Generate the data.yaml configuration file required by YOLO
        yaml_path = os.path.join(DATASET_DIR, 'data_custom.yaml')
        yaml_content = {
            'path': DATASET_DIR,
            'train': train_dir,
            'val': val_dir,
            'nc': len(CLASSES),     # Automatically calculate number of classes (4)
            'names': CLASSES        # Map the class names
        }

        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f)

        print(f"‚úÖ Configuration file generated successfully with {len(CLASSES)} classes.")

        # Begin Training
        print(f"üî• Starting model training (Max 200 epochs)...")
        try:
            model = YOLO('yolov8n.pt') # Initialize YOLOv8 Nano for object detection

            model.train(
                data=yaml_path,
                epochs=200,           # Maximum training epochs
                patience=20,          # Stop early if no improvement after 20 epochs
                imgsz=640,
                plots=False,
                name="train_food_all" # Directory name for saved run logs
            )

            # Extract the best model weights
            best_weights = glob.glob('/content/runs/detect/train_food_all*/weights/best.pt')

            if best_weights:
                # Get the most recently generated model
                target = max(best_weights, key=os.path.getctime)
                final_path = '/content/model_mixed_rice.pt'
                shutil.copy(target, final_path)
                print(f"\nüéâ Training successful! Unified model saved at: {final_path}")
                print("You can now download this model for inference testing in the main system!")
            else:
                print("\n‚ùå Training completed, but weights/best.pt was not found in the runs directory.")

        except Exception as e:
            print(f"\n‚ùå An error occurred during training: {e}")

# ==============================================================================
# üöÄ Clean Edition: RT-DETR Automated Training Script (Training Only)
# ==============================================================================
import os
import shutil
import glob
from ultralytics import RTDETR

print(f"\n{'='*80}")
print("üî• Starting RT-DETR Pure Training Pipeline")
print(f"{'='*80}")

# --- Configuration ---
yaml_path = '/content/food_dataset/data_custom.yaml'
best_rtdetr_model_path = '/content/model_rtdetr_mixed_rice.pt'

if not os.path.exists(yaml_path):
    print(f"‚ùå Error: {yaml_path} not found! Please ensure the dataset is extracted and the YAML is generated.")
else:
    print("üîÑ Loading RT-DETR pre-trained Large model (rtdetr-l.pt)...")
    try:
        # Initialize the RT-DETR model (Transformer-based architecture)
        model = RTDETR('rtdetr-l.pt')

        # Begin Training
        print("üî• Starting model training (Max 200 epochs)...")
        model.train(
            data=yaml_path,
            epochs=200,
            patience=20,          # üåü IMPROVEMENT: Added Early Stopping to prevent overfitting/wasting time!
            imgsz=640,
            plots=False,
            name="train_food_rtdetr"
        )

        # Extract and save the best model weights
        best_weights = glob.glob('/content/runs/detect/train_food_rtdetr*/weights/best.pt')

        if best_weights:
            # Locate the most recently generated 'best.pt' file
            target = max(best_weights, key=os.path.getctime)
            shutil.copy(target, best_rtdetr_model_path)
            print(f"\nüéâ RT-DETR Training Successful! Model saved as: {best_rtdetr_model_path}")
        else:
            print("\n‚ùå Training completed, but weights/best.pt was not found.")

    except Exception as e:
        print(f"\n‚ùå RT-DETR Training Error: {e}")

# ==============================================================================
# üöÄ Module 1 & 2: Faster R-CNN PyTorch Training & Inference Pipeline
# ==============================================================================
import os
import zipfile
import glob
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from google.colab import files

print(f"\n{'='*60}")
print("üî• Initializing Faster R-CNN (Two-Stage Detector) System")
print(f"{'='*60}")

# ---------------------------------------------------------
# ‚öôÔ∏è Configuration Area
# ---------------------------------------------------------
DATASET_NAME = 'food_dataset'
ZIP_PATH = f'/content/{DATASET_NAME}.zip'
DATASET_DIR = f'/content/{DATASET_NAME}'

# Mapping matches your previous setup
CLASSES = ['meat', 'plate', 'rice', 'vege']
NUM_CLASSES = len(CLASSES) + 1  # PyTorch requires +1 for the 'background' class

BASE_PRICES = {'meat': 4.00, 'rice': 1.50, 'vege': 2.00}
CURRENCY = "RM"
SIZE_MULTIPLIERS = {'S': 0.7, 'M': 1.0, 'L': 1.5}
COLORS = {'rice': (0, 255, 0), 'vege': (255, 0, 0), 'meat': (0, 0, 255), 'plate': (0, 255, 255)}

# --- 1. Dataset Extraction ---
if not os.path.exists(ZIP_PATH):
    print(f"‚ùå Error: {ZIP_PATH} not found! Please upload your dataset zip to the left panel.")
else:
    if not os.path.exists(DATASET_DIR):
        print(f"üì¶ Extracting {DATASET_NAME}.zip ...")
        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
            zip_ref.extractall(DATASET_DIR)

# Find train images (dynamically locates the folder)
train_images_dir = glob.glob(f"{DATASET_DIR}/**/train/images", recursive=True)[0]
train_labels_dir = train_images_dir.replace('images', 'labels')

# --- 2. Custom PyTorch Dataset (Robust YOLO to Faster R-CNN format) ---
class YOLODataset(Dataset):
    def __init__(self, img_dir, label_dir):
        self.img_dir = img_dir
        self.label_dir = label_dir
        self.imgs = [img for img in os.listdir(img_dir) if img.endswith(('.jpg', '.png', '.jpeg'))]

    def __getitem__(self, idx):
        img_name = self.imgs[idx]
        img_path = os.path.join(self.img_dir, img_name)

        # Load image
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        height, width, _ = img.shape
        img_tensor = torch.as_tensor(img, dtype=torch.float32).permute(2, 0, 1) / 255.0

        # Load labels
        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + '.txt')
        boxes, labels = [], []

        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                for line in f.readlines():
                    parts = line.strip().split()
                    if not parts: continue

                    class_id = int(parts[0])

                    if len(parts) == 5:
                        # Format 1: Standard YOLO Bounding Box (class, xc, yc, w, h)
                        _, x_center, y_center, w, h = map(float, parts)
                        xmin = (x_center - w/2) * width
                        xmax = (x_center + w/2) * width
                        ymin = (y_center - h/2) * height
                        ymax = (y_center + h/2) * height
                    else:
                        # Format 2: YOLO Segmentation Polygon -> Convert to Bounding Box
                        coords = list(map(float, parts[1:]))
                        x_coords = coords[0::2]
                        y_coords = coords[1::2]

                        xmin = min(x_coords) * width
                        xmax = max(x_coords) * width
                        ymin = min(y_coords) * height
                        ymax = max(y_coords) * height

                    # Ensure bounding box coordinates stay within image boundaries
                    xmin, ymin = max(0, xmin), max(0, ymin)
                    xmax, ymax = min(width, xmax), min(height, ymax)

                    # Only append if valid dimensions
                    if xmax > xmin and ymax > ymin:
                        boxes.append([xmin, ymin, xmax, ymax])
                        labels.append(class_id + 1) # Shift class ID by 1 for background

        target = {}
        target["boxes"] = torch.as_tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 4), dtype=torch.float32)
        target["labels"] = torch.as_tensor(labels, dtype=torch.int64) if len(labels) > 0 else torch.zeros((0,), dtype=torch.int64)
        return img_tensor, target

    def __len__(self):
        return len(self.imgs)

# Helper function to collate batches for DataLoader
def collate_fn(batch): return tuple(zip(*batch))

# --- 3. Model Setup & Training Loop ---
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print(f"‚ö° Using device: {device}")

# Load pre-trained Faster R-CNN with ResNet50 backbone
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights="DEFAULT")
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)
model.to(device)

print("\nüöÄ Preparing Dataset Loaders...")
train_dataset = YOLODataset(train_images_dir, train_labels_dir)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

# Keep epochs relatively low for initial testing
EPOCHS = 200
print(f"üî• Starting PyTorch Training Loop for {EPOCHS} Epochs...")

for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0
    for images, targets in train_loader:
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()
        epoch_loss += losses.item()

    print(f"Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss/len(train_loader):.4f}")

torch.save(model.state_dict(), '/content/faster_rcnn_mixed_rice.pth')
print("‚úÖ Training Complete! Model saved as faster_rcnn_mixed_rice.pth")

# ==============================================================================
# ‚öñÔ∏è Ultimate Edition 3.1: Tri-Model Smart Cashier System (Bug Fixes Applied)
# Models: YOLO (1-stage), RT-DETR (Transformer), Faster R-CNN (2-stage)
# ==============================================================================
import cv2
import matplotlib.pyplot as plt
import os
import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
from ultralytics import YOLO, RTDETR
from google.colab import files

print(f"\n{'='*60}")
print("‚öñÔ∏è Starting Ultimate Tri-Model Smart Cashier System")
print(f"{'='*60}")

# --- 1. Basic Configuration ---
BASE_PRICES = {'meat': 4.00, 'rice': 1.50, 'vege': 2.00}
CURRENCY = "RM"
SIZE_MULTIPLIERS = {'S': 0.7, 'M': 1.0, 'L': 1.5}

# Unified class mapping
CLASSES = ['meat', 'plate', 'rice', 'vege']
COLORS = {'rice': (0, 255, 0), 'vege': (255, 0, 0), 'meat': (0, 0, 255), 'plate': (0, 255, 255)}

# --- 2. Portion Logic (Ratio-Based) ---
def get_portion_size(box_area, plate_area):
    if plate_area <= 0: return 'M'
    ratio = box_area / plate_area
    if ratio < (2 / 9): return 'S'
    elif ratio > (4 / 9): return 'L'
    else: return 'M'

# --- 3. Pre-load All AI Models ---
print("üîÑ Loading the AI models into memory (This may take a moment)...")

# 3a. YOLO
yolo_path = '/content/model_mixed_rice.pt'
yolo_model = YOLO(yolo_path) if os.path.exists(yolo_path) else None
print("‚úÖ YOLO Model loaded!" if yolo_model else "‚ùå YOLO Model not found.")

# 3b. RT-DETR
rtdetr_path = '/content/model_rtdetr_mixed_rice.pt'
rtdetr_model = RTDETR(rtdetr_path) if os.path.exists(rtdetr_path) else None
print("‚úÖ RT-DETR Model loaded!" if rtdetr_model else "‚ùå RT-DETR Model not found.")

# 3c. Faster R-CNN (PyTorch)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
frcnn_path = '/content/faster_rcnn_mixed_rice.pth'
frcnn_model = None
if os.path.exists(frcnn_path):
    frcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)
    in_features = frcnn_model.roi_heads.box_predictor.cls_score.in_features
    frcnn_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(CLASSES) + 1)
    frcnn_model.load_state_dict(torch.load(frcnn_path, map_location=device))
    frcnn_model.to(device)
    frcnn_model.eval()
    print("‚úÖ Faster R-CNN Model loaded!")
else:
    print("‚ùå Faster R-CNN Model not found.")

# --- 4. Core Interactive Loop ---
current_image_path = None

while True:
    print("\n" + "="*50)
    print("Please select the AI algorithm for pricing:")
    print("1: YOLO (CNN Single-stage - Max Speed)")
    print("2: RT-DETR (Transformer - Global Attention)")
    print("3: Faster R-CNN (Two-stage CNN - High Accuracy)")
    print("4: Exit System and clear cache")

    choice = input("üëâ Enter your choice (1/2/3/4): ").strip()

    if choice == '4':
        print("üëã Thank you for using the Zapfan Smart Cashier. Exiting...")
        if current_image_path and os.path.exists(current_image_path):
            os.remove(current_image_path)
        break

    if choice == '1' and not yolo_model:
        print("‚ö†Ô∏è YOLO model is not loaded!"); continue
    elif choice == '2' and not rtdetr_model:
        print("‚ö†Ô∏è RT-DETR model is not loaded!"); continue
    elif choice == '3' and not frcnn_model:
        print("‚ö†Ô∏è Faster R-CNN model is not loaded!"); continue
    elif choice not in ['1', '2', '3']:
        print("‚ö†Ô∏è Invalid input. Please enter 1, 2, 3, or 4."); continue

    model_names = {'1': 'YOLO', '2': 'RT-DETR', '3': 'Faster R-CNN'}
    selected_name = model_names[choice]

    if choice == '1': active_model = yolo_model
    elif choice == '2': active_model = rtdetr_model
    else: active_model = frcnn_model

    # =====================================================
    # üì∏ Smart Image Upload Logic
    # =====================================================
    if current_image_path and os.path.exists(current_image_path):
        reuse = input(f"üîÑ Reuse the previous image for [{selected_name}] testing? (y/n, default y): ").strip().lower()
        if reuse == 'n':
            os.remove(current_image_path)
            current_image_path = None

    if not current_image_path:
        print(f"\nüì∏ Please upload a picture of the mixed rice for [{selected_name}] analysis...")
        try:
            uploaded = files.upload()
            if not uploaded:
                print("‚ö†Ô∏è No image uploaded. Returning to main menu.")
                continue
            current_image_path = list(uploaded.keys())[0]
        except Exception as e:
            print(f"‚ùå Upload error: {e}")
            continue

    # =====================================================
    # üß† Start Inference Branching
    # =====================================================
    try:
        # --- FIX: Safe Image Loading Check ---
        img = cv2.imread(current_image_path)
        if img is None:
            print(f"‚ùå Error: Could not read image file at '{current_image_path}'. The file might be corrupted or missing.")
            current_image_path = None
            continue

        img_display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_clean = img_display.copy()

        valid_foods = []
        plate_box = None
        highest_plate_conf = 0

        # --- BRANCH A: Ultralytics Logic (YOLO / RT-DETR) ---
        if choice in ['1', '2']:
            results = active_model.predict(img_display, conf=0.25, verbose=False)
            for r in results:
                for box in r.boxes:
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    conf = float(box.conf[0])
                    cls_id = int(box.cls[0])
                    task_name = CLASSES[cls_id]

                    if task_name == 'plate':
                        if conf > highest_plate_conf:
                            highest_plate_conf = conf
                            plate_box = (x1, y1, x2, y2)
                    else:
                        valid_foods.append({'name': task_name, 'box': (x1, y1, x2, y2), 'score': conf})

        # --- BRANCH B: PyTorch Logic (Faster R-CNN) ---
        elif choice == '3':
            img_tensor = torch.as_tensor(img_display, dtype=torch.float32).permute(2, 0, 1) / 255.0
            img_tensor = img_tensor.unsqueeze(0).to(device)

            with torch.no_grad():
                predictions = active_model(img_tensor)[0]

            CONF_THRESH = 0.5
            mask = predictions['scores'] > CONF_THRESH
            boxes = predictions['boxes'][mask].cpu().numpy()
            labels = predictions['labels'][mask].cpu().numpy()
            scores = predictions['scores'][mask].cpu().numpy()

            for i, box in enumerate(boxes):
                x1, y1, x2, y2 = map(int, box)
                cls_id = labels[i] - 1
                task_name = CLASSES[cls_id]
                conf = scores[i]

                if task_name == 'plate':
                    if conf > highest_plate_conf:
                        highest_plate_conf = conf
                        plate_box = (x1, y1, x2, y2)
                else:
                    valid_foods.append({'name': task_name, 'box': (x1, y1, x2, y2), 'score': conf})

        # =====================================================
        # Unified Downstream Processing
        # =====================================================
        if not valid_foods:
            print(f"‚ö†Ô∏è [{selected_name}] could not detect any food. Try another image.")
            os.remove(current_image_path)
            current_image_path = None
            continue

        # üçΩÔ∏è Plate Calculation
        if plate_box is not None:
            px1, py1, px2, py2 = plate_box
            plate_area = max(1, (px2 - px1) * (py2 - py1))
            cv2.rectangle(img_display, (px1, py1), (px2, py2), COLORS['plate'], 3)
            cv2.putText(img_display, f"Plate ({highest_plate_conf:.2f})", (px1, py1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, COLORS['plate'], 2)
        else:
            print("‚ö†Ô∏è Plate not detected. Estimating plate from food boundaries...")
            min_x = min([d['box'][0] for d in valid_foods])
            min_y = min([d['box'][1] for d in valid_foods])
            max_x = max([d['box'][2] for d in valid_foods])
            max_y = max([d['box'][3] for d in valid_foods])
            plate_area = max(1, (max_x - min_x) * (max_y - min_y))
            # --- FIX: Replaced LINE_DASH with LINE_AA ---
            cv2.rectangle(img_display, (min_x, min_y), (max_x, max_y), (255, 255, 255), 2, cv2.LINE_AA)

        # üí∞ Pricing, Drawing, and Cropping
        total_bill = 0.0
        receipt_lines = []
        cropped_images = []

        for item in valid_foods:
            task_name, conf, (x1, y1, x2, y2) = item['name'], item['score'], item['box']
            box_area = (x2 - x1) * (y2 - y1)
            size_label = get_portion_size(box_area, plate_area)

            multiplier = SIZE_MULTIPLIERS[size_label]
            final_price = BASE_PRICES[task_name] * multiplier
            total_bill += final_price

            ratio_percentage = (box_area / plate_area) * 100
            receipt_lines.append(f"{task_name.capitalize()} ({size_label}, {ratio_percentage:.1f}%): {CURRENCY}{final_price:.2f}")

            # ‚úÇÔ∏è Clean Crop Logic
            crop_y1, crop_y2 = max(0, y1), min(img_clean.shape[0], y2)
            crop_x1, crop_x2 = max(0, x1), min(img_clean.shape[1], x2)
            if crop_y2 > crop_y1 and crop_x2 > crop_x1:
                cropped_part = img_clean[crop_y1:crop_y2, crop_x1:crop_x2].copy()
                cropped_images.append((task_name, conf, cropped_part))

            # Draw standard bounding boxes on Display Image
            color = COLORS[task_name]
            rgb = (color[2], color[1], color[0])
            cv2.rectangle(img_display, (x1, y1), (x2, y2), rgb, 3)

            label = f"{task_name.capitalize()} ({size_label}) {CURRENCY}{final_price:.2f}"
            t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(img_display, (x1, max(y1 - t_size[1] - 10, 0)), (x1 + t_size[0], max(y1, 10)), rgb, -1)
            cv2.putText(img_display, label, (x1, max(y1 - 5, 15)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # 1. Display Result Image
        plt.figure(figsize=(10, 10))
        plt.imshow(img_display)
        plt.axis('off')
        plt.title(f"{selected_name} Total: {CURRENCY}{total_bill:.2f}", fontsize=18, color='green', fontweight='bold')
        plt.show()

        # 2. Display Cropped Images Side-by-Side
        if cropped_images:
            print("\n" + "‚úÇÔ∏è "*15)
            print(f" EXTRACTED CROP ITEMS ({selected_name})")
            print("‚úÇÔ∏è "*15)
            num_crops = min(len(cropped_images), 8)
            fig, axes = plt.subplots(1, num_crops, figsize=(3 * num_crops, 3))
            if num_crops == 1: axes = [axes]
            for i in range(num_crops):
                c_name, c_score, c_img = cropped_images[i]
                axes[i].imshow(c_img)
                axes[i].axis('off')
                axes[i].set_title(f"{c_name.upper()}\nConf: {c_score:.2f}", color='blue', fontweight='bold')
            plt.show()

        # 3. Print Receipt
        print(f"\nüßæ Smart Receipt [{selected_name}]:")
        for line in receipt_lines:
            print(f"   {line}")
        print(f"   {'='*35}")
        print(f"   TOTAL: {CURRENCY}{total_bill:.2f}\n")

    except Exception as e:
        print(f"‚ùå An error occurred during execution: {e}")